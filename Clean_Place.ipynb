{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clean Place.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reallygooday/60daysofudacity/blob/master/Clean_Place.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpMRVjIn76Vj",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/1iDYWfzeX9sAU7SUnqPc2oXiwtD92YxCb#scrollTo=LpMRVjIn76Vj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjfzYkdbpxlp",
        "colab_type": "code",
        "outputId": "da8e7390-477c-47f9-ee24-e3ed6d86386c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1225865b-a882-4331-bd1e-e71fe7c74e54",
        "id": "blVgZ_jUw1T7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/Text Generator'"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " oliver.txt  'preprocessed2019-07-30 17:06:00.732826.p'\n",
            " place2.txt  'preprocessed2019-07-30 17:06:42.389736.p'\n",
            " place.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUIz-wk13IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('place.txt', 'r') as f:\n",
        "  transcript = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0j2Hf8t2UmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = transcript.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LDpq6VT13xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "line_lengths = [len(line.split()) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dHSOUP63pB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getUniqueWords(text):\n",
        "  words = text.lower().split()\n",
        "  return list(set(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9BgEp8N3pVC",
        "colab_type": "code",
        "outputId": "6c10a78f-0cd2-4c0c-bde9-c95ead831827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "print('number of lines: ', len(lines))\n",
        "print('average number of words in a sentence: ', round(np.average(line_lengths), 1))\n",
        "print('number of words: ', len(transcript.lower().split()))\n",
        "print('number of unique words: ', len(getUniqueWords(transcript)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of lines:  1685\n",
            "average number of words in a sentence:  8.3\n",
            "number of words:  13937\n",
            "number of unique words:  3051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZwv26eO3pqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getUniqueChars(text):\n",
        "  uniqueChars = list(set([char for char in text.lower()]))\n",
        "  uniqueChars.sort()\n",
        "  return uniqueChars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFZaqgW614Dx",
        "colab_type": "code",
        "outputId": "e7495ed4-5d79-418a-b010-71375b49c132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print('There are {} unique characters in the dataset, counting lower and upper case letters as one letter'.format(len(getUniqueChars(transcript))))\n",
        "print(getUniqueChars(transcript))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 65 unique characters in the dataset, counting lower and upper case letters as one letter\n",
            "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', '—', '’', '“', '”', '\\ufeff']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw1oTPqJ38T9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "def cleanupText(text):\n",
        "  wanted_chars = string.digits + string.ascii_letters + \"'\" + '\".,:;!?-() \\n'\n",
        "  print(wanted_chars)\n",
        "  return ''.join(x for x in text if x in (wanted_chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpQlkeIP4BTa",
        "colab_type": "code",
        "outputId": "373102af-bc30-4f3f-f792-ef2e4dff4e71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "transcript = cleanupText(transcript)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\".,:;!?-() \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnex6e3Q4Bm_",
        "colab_type": "code",
        "outputId": "7aff733e-1ed0-4fc5-801c-f178a6824f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print('After cleanup there are {} unique characters in the dataset'.format(len(getUniqueChars(transcript))))\n",
        "print(getUniqueChars(transcript))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After cleanup there are 49 unique characters in the dataset\n",
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH_33xCg4B4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuation_dict = {\n",
        "        '.':'||period||',\n",
        "        ',':'||comma||',\n",
        "        '\"':'||quote||',\n",
        "        ';':'||semicolon||',\n",
        "        '!':'||exclamation||',\n",
        "        '?':'||question||',\n",
        "        '(':'||leftPar||',\n",
        "        ')':'||rightPar||',\n",
        "        '-':'||dash||',\n",
        "        '\\n':'||return||',\n",
        "        \"'\":'||single_quote||'\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNGdG0l638lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizePunctuation(text):\n",
        "  for key, value in punctuation_dict.items():\n",
        "    text = text.replace(key, ' {} '.format(value))\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHmKr-Vp4RAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transcript = tokenizePunctuation(transcript)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUnFQMCU4RSs",
        "colab_type": "code",
        "outputId": "0ac2065c-24a7-431a-cf94-dfafdfa8b756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(transcript[:100])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of Three Stories  Ten Poems ||comma||  by Ernest Hemingway ||return||  |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idGSER7D4Rp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_words = getUniqueWords(transcript)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db1Nwxsl383Z",
        "colab_type": "code",
        "outputId": "fa5632f0-1089-4c6f-899e-a7911219c456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('The number of unique words is: ', len(unique_words))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique words is:  2391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SeOFYN54mvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def createLookupTables(text, padding_symbol):\n",
        "  text = text.lower().split()\n",
        "  text.append(padding_symbol)\n",
        "  count = Counter(text)\n",
        "  vocabulary = sorted(count, key=count.get, reverse=True)\n",
        "  word_to_int = {word:idx for idx, word in enumerate(vocabulary)}\n",
        "  int_to_word = {idx:word for idx, word in enumerate(vocabulary)}\n",
        "  return (word_to_int,int_to_word)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXDJNYby4pKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PADDING_SYMBOL = '<PAD>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgVnyZIn4pf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_int,int_to_word = createLookupTables(transcript, PADDING_SYMBOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKxPTAHB4nBC",
        "colab_type": "code",
        "outputId": "74979ab8-fa7e-4e32-ffff-19917888804f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(int_to_word[i])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "||return||\n",
            "||period||\n",
            "the\n",
            "and\n",
            "||comma||\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g38nRoJC402s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def changeIntoInts(text):\n",
        "  return [word_to_int[word] for word in text.lower().split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt6DnjhE41Ik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transcript_int = changeIntoInts(transcript)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSQbsIzJ41bH",
        "colab_type": "code",
        "outputId": "ff6164d5-4e58-4c1f-f897-fe322be842fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(transcript_int[:20])"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 22, 20, 172, 5, 87, 256, 203, 234, 4, 41, 470, 471, 0, 0, 33, 172, 36, 24, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSAhGWyC5Cti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB8wEs1Rc51F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "\n",
        "project_dir = \"/content/drive/My Drive/Text Generator/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMZyrWI25NHk",
        "colab_type": "code",
        "outputId": "4943bbc2-11d7-458c-af30-4e79eae60a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#saving current preprocessed data\n",
        "path = project_dir + 'preprocessed' + str(datetime.datetime.now()) +'.p'\n",
        "save_preprocessed(word_to_int, int_to_word, punctuation_dict, transcript_int, path)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved preprocessed data in /content/drive/My Drive/Text Generator/preprocessed2019-07-30 17:14:12.698497.p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKGsNSbv5C_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_preprocessed(word_to_int, int_to_word, punctuation_dict, transcript_int, path):  \n",
        "  with open(path, 'wb') as f:\n",
        "    pickle.dump((word_to_int, int_to_word, punctuation_dict, transcript_int), f)\n",
        "    print('Saved preprocessed data in', path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XV7T4bl5DQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_preprocessed(path):\n",
        "  return pickle.load(open(path, mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAV7lePL66lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "def createDataloader(text_int, sequence_length, batch_size):\n",
        "  \n",
        "  #first we need to convert the whole dataset into features and targets lists\n",
        "  features, target = [], []\n",
        "  \n",
        "  #we loop through the whole dataset, moving one element at a time\n",
        "  #we start at the first element and end at the element that is sequence_length from the end of the dataset\n",
        "  \n",
        "  for sequence_no in range(len(text_int)-sequence_length):\n",
        "    features.append(text_int[sequence_no:sequence_no+sequence_length])\n",
        "    target.append(text_int[sequence_no+sequence_length])\n",
        "    \n",
        "  #then we convert the dataset into the Tensor Dataset, which accepts NumPy arrays as parameter\n",
        "  #Tensor Dataset needs features and target nparrays as parameters\n",
        "  \n",
        "  dataset = TensorDataset(torch.from_numpy(np.array(features)), torch.from_numpy(np.array(target)))\n",
        "\n",
        "  #once we have the appropriate dataset, we can create the dataloder\n",
        "  \n",
        "  dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
        "  \n",
        "  return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiUutag5663O",
        "colab_type": "code",
        "outputId": "aa379c86-e4f4-4d28-a42a-4ddc6fc6b126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "sample_loader = createDataloader(transcript_int[:50], sequence_length=5, batch_size=10)\n",
        "sample_iterable = iter(sample_loader)\n",
        "sample_feature, sample_target = sample_iterable.next()\n",
        "\n",
        "print('Sample feature batch:')\n",
        "print(sample_feature)\n",
        "print('Sample target:')\n",
        "print(sample_target)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample feature batch:\n",
            "tensor([[  0,  76, 598,   5,   2],\n",
            "        [  2,  22,  20, 172,   5],\n",
            "        [  3, 235,   0,  76, 598],\n",
            "        [ 24,   2, 147,   5, 413],\n",
            "        [105,   3, 235,   0,  76],\n",
            "        [787,   0, 788,   1,  13],\n",
            "        [413, 597,   8,   2, 139],\n",
            "        [ 33, 172,  36,  24,   2],\n",
            "        [ 36,  24,   2, 147,   5],\n",
            "        [ 76, 598,   5,   2, 414]])\n",
            "Sample target:\n",
            "tensor([414,  87,   5, 597, 598, 119, 105, 147, 413,  15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0clhlRb667Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ScriptGenModel(nn.Module):\n",
        "  \n",
        "  #here we define the structure of the model\n",
        "  \n",
        "  def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
        "    super(ScriptGenModel, self).__init__()\n",
        "    \n",
        "    #saving the parameters for future use\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_dim = hidden_dim\n",
        "        \n",
        "    # defining model layers\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "  #here we decide, how the forward pass of the model should be performed\n",
        "  \n",
        "  def forward(self, nn_input, hidden):\n",
        "    \n",
        "    batch_size = nn_input.size(0)\n",
        "\n",
        "    embeds = self.embedding(nn_input)\n",
        "    lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "    out = self.fc(lstm_out)\n",
        "\n",
        "    out = out.view(batch_size, -1, self.output_size)\n",
        "    out = out[:, -1]\n",
        "    return out, hidden\n",
        "  \n",
        "  #here we define the way to initialise the hidden state\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "    if (train_on_gpu):\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "    else:\n",
        "      hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e_qKNAO5NYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_back_prop(scriptGenModel, optimizer, criterion, inp, target, hidden):\n",
        "  # move data to GPU, if available\n",
        "  if(train_on_gpu):\n",
        "    inp, target = inp.cuda(), target.cuda()\n",
        "    \n",
        "  # perform backpropagation and optimization\n",
        "  hidden = tuple([each.data for each in hidden])\n",
        "\n",
        "  scriptGenModel.zero_grad()\n",
        "    \n",
        "  output, hidden = scriptGenModel(inp, hidden)\n",
        "\n",
        "  loss = criterion(output, target)\n",
        "  loss.backward()\n",
        "  nn.utils.clip_grad_norm_(scriptGenModel.parameters(), 3)\n",
        "  optimizer.step()\n",
        "  return loss.item(), hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqK7KEa85Npu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_scriptGenModel(scriptGenModel, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
        "    batch_losses = []\n",
        "    \n",
        "    scriptGenModel.train()\n",
        "\n",
        "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
        "    for epoch_i in range(1, n_epochs + 1):\n",
        "        \n",
        "        # initialize hidden state\n",
        "        hidden = scriptGenModel.init_hidden(batch_size)\n",
        "        \n",
        "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
        "            \n",
        "            # make sure you iterate over completely full batches, only\n",
        "            n_batches = len(train_loader.dataset)//batch_size\n",
        "            if(batch_i > n_batches):\n",
        "                break\n",
        "            \n",
        "            # forward, back prop\n",
        "            loss, hidden = forward_back_prop(scriptGenModel, optimizer, criterion, inputs, labels, hidden)          \n",
        "            # record loss\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "            # printing loss stats\n",
        "            if batch_i % show_every_n_batches == 0:\n",
        "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
        "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
        "                batch_losses = []\n",
        "\n",
        "    # returns a trained rnn\n",
        "    return scriptGenModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM3w25Py7UrI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters for data preprocessing\n",
        "\n",
        "sequence_length = 8\n",
        "batch_size = 64\n",
        "\n",
        "# Training parameters\n",
        "\n",
        "num_epochs = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model parameters\n",
        "\n",
        "vocab_size = len(word_to_int)\n",
        "output_size = vocab_size\n",
        "embedding_dim = 300\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "# Show stats for every n number of batches\n",
        "\n",
        "show_every_n_batches = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqfP3HYo7U75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(filename, scriptGenModel):\n",
        "  model_save_name = filename + \".pt\"\n",
        "  path = project_dir + model_save_name\n",
        "  torch.save(scriptGenModel, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUFoleFr7VM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(filename):\n",
        "    path = project_dir + filename + \".pt\"\n",
        "    return torch.load(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJDb4uM7hjv",
        "colab_type": "code",
        "outputId": "71112a13-b08c-4ebc-e0db-2e1af8008b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#checking if GPU is available\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if not train_on_gpu:\n",
        "  print('There is no GPU available. Please consider switching to GPU for training the model.')\n",
        "else:\n",
        "  print(\"GPU available. You're good to go!\") "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available. You're good to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M90pTqbr71BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = createDataloader(transcript_int, sequence_length, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z14C2I6n7h1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creaing the model\n",
        "scriptGenModel = ScriptGenModel(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
        "\n",
        "#deciding on optimizer and loss functions\n",
        "optimizer = torch.optim.Adam(scriptGenModel.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E57oiVck4nSj",
        "colab_type": "code",
        "outputId": "8e3b1d78-405b-438b-cdb2-920c00392ed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(scriptGenModel)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ScriptGenModel(\n",
            "  (embedding): Embedding(2392, 300)\n",
            "  (lstm): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=2392, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkomUKF99kH3",
        "colab_type": "code",
        "outputId": "1ae25634-3901-4deb-f084-3d5027349aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#moving the model to GPU if it's available\n",
        "if train_on_gpu:\n",
        "    scriptGenModel.cuda()\n",
        "\n",
        "#running the trainining loop\n",
        "trained_scriptGenModel = train_scriptGenModel(scriptGenModel, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
        "\n",
        "#Setup for saving the trained model to Google Drive\n",
        "current_time = datetime.datetime.now() \n",
        "model_name = 'trained_scriptGenModel' + str(current_time)\n",
        "save_model(model_name, trained_scriptGenModel)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 10 epoch(s)...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ScriptGenModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SI2iGlk9kab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(scriptGenModel, prime_id, int_to_vocab, punctuation_dict, pad_value, predict_len=100):\n",
        "\n",
        "    scriptGenModel.eval()\n",
        "    \n",
        "    # create a sequence (batch_size=1) with the prime_id\n",
        "    current_seq = np.full((1, sequence_length), pad_value)\n",
        "    current_seq[-1][-1] = prime_id\n",
        "    predicted = [int_to_vocab[prime_id]]\n",
        "    \n",
        "    for _ in range(predict_len):\n",
        "        if train_on_gpu:\n",
        "            current_seq = torch.LongTensor(current_seq).cuda()\n",
        "        else:\n",
        "            current_seq = torch.LongTensor(current_seq)\n",
        "        \n",
        "        # initialize the hidden state\n",
        "        hidden = scriptGenModel.init_hidden(current_seq.size(0))\n",
        "        \n",
        "        # get the output of the model\n",
        "        \n",
        "        ##### Added next 2 lines to fix numpy bug  ####\n",
        "        scriptGenModel.cpu()\n",
        "        current_seq = current_seq.cpu()\n",
        "        hidden = hidden[0].cpu(), hidden[1].cpu()\n",
        "        #print(\" curs:\", current_seq.device, \"hidden[0]\", hidden[0].device, \"hidden[1]\", hidden[1].device)\n",
        "        output, _ = scriptGenModel(current_seq, hidden)\n",
        "        \n",
        "        # get the next word probabilities\n",
        "        p = F.softmax(output, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "         \n",
        "        # use top_k sampling to get the index of the next word\n",
        "        top_k = 5\n",
        "        p, top_i = p.topk(top_k)\n",
        "        top_i = top_i.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next word index with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
        "        \n",
        "        # retrieve that word from the dictionary\n",
        "        word = int_to_vocab[word_i]\n",
        "        predicted.append(word)     \n",
        "        \n",
        "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
        "        current_seq = np.roll(current_seq, -1, 1)\n",
        "        current_seq[-1][-1] = word_i\n",
        "    \n",
        "    gen_sentences = ' '.join(predicted)\n",
        "    \n",
        "    # Replace punctuation tokens\n",
        "    for key, token in punctuation_dict.items():\n",
        "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
        "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
        "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
        "    gen_sentences = gen_sentences.replace('( ', '(')\n",
        "    \n",
        "    # return all the sentences\n",
        "    return gen_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvzSPBGO9kxK",
        "colab_type": "code",
        "outputId": "93aaeedd-e414-45fb-9dc6-45070b0493d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#we check if the parameters for text generation are loaded, and if not we load them in\n",
        "\n",
        "try:\n",
        "  if (word_to_int and int_to_word and punctuation_dict and PADDING_SYMBOL):\n",
        "    print('Data for text generation present')\n",
        "except:\n",
        "  path = project_dir+'preprocessed.p'\n",
        "  word_to_int, int_to_word, punctuation_dict, transcript_int = load_preprocessed(path)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data for text generation present\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZIUMnae90qo",
        "colab_type": "code",
        "outputId": "60720a77-766d-4de9-ba8f-34baa426a2a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#we check if the model is present, and if not we load it in\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  if (trained_scriptGenModel):\n",
        "    print('The model for text generation is present')\n",
        "except:\n",
        "  trained_scriptGenModel = load_model('trained_scriptGenModel')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model for text generation is present\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hdks7hP96_G",
        "colab_type": "code",
        "outputId": "21987fb9-39a6-41e3-cdbf-49c9ed98aa45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "gen_length = 500 \n",
        "prime_word = 'What'\n",
        "\n",
        "generated_script = generate(trained_scriptGenModel, word_to_int[prime_word.lower()], int_to_word, punctuation_dict, word_to_int[PADDING_SYMBOL], gen_length)\n",
        "print(generated_script)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what but i couldnt help feeling my old man\n",
            "and the owner of a state and then the\n",
            "dew all over the side of the pack and\n",
            "the pesage. my old man had just weighed in too and came out with the wagon\n",
            "and my old man had bet for his pocket in the letter rack at the dome)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "along with youth\n",
            "chapter heading\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "up at michigan\n",
            "and four ashes. he talked his leader at the tree.\n",
            "he felt hurt. she was excited frightened\n",
            "and didnt afraid to go up the list.\n",
            "\n",
            "that damn big day jimmy was one for a bottle. she was being led\n",
            "around the paddock with his head down and then they\n",
            "come by my old man said you had him, he was a nice investment was a good investment jumper the\n",
            "whole way, especially up over to the\n",
            "road. i was fond about the horses. he was quite a sharp, aching, hurting feeling that\n",
            "they thought that she thought it. she liked it all\n",
            "the winning stall and they went out of the\n",
            "street of mississippi. if you are not located in the united states, you may return a refund in writing\n",
            "number, is all cleaned up frozen\n",
            "manure with the padrone at the hotel office. it was\n",
            "frightened, said the y. g. went on\n",
            "ahead. he was being bi of her. she was being frightened and i couldnt help feeling that\n",
            "if my old man had a lot of little to watch them. he was dropping\n",
            "money every day at the river. it was a great day after the\n",
            "jock was sitting up and there was quite a wagon hitched up the hill. it was a good dinner. the\n",
            "men ate seriously. there was farming country and timber each way up\n",
            "the road. she was being asleep. she wanted it. she liked it all right. she liked it all. it would have brought him, he said, slapping the y. g. and unjointed the rod, reeling the\n",
            "line on their stone. he was being led and down at the\n",
            "old guy was a dead day.\n",
            "\n",
            "i dont know where he is, he said, slapping the y. g. went on and the\n",
            "men ate seriously. after smiths back door she could see her barges\n",
            "around and the barrier snapping up and the stable rang to watch on the\n",
            "post and beating it out to the stables in front,\n",
            "the moon rises,\n",
            "ponies\n",
            "drag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XerZQfgk97Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveGenScript(generated_script):\n",
        "  path = project_dir + 'generated_script' + str(datetime.datetime.now()) + '.txt'\n",
        "  with open(path, \"w\") as text_file:\n",
        "    text_file.write(generated_script)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVAg1cJl97mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saveGenScript(generated_script)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_Wqa_j_9075",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}